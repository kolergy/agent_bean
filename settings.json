{
    "description":"Agent_Bean settings",
    "debug": true,
    "reload_models": false,
    "known_models_file_name": "known_models.json",
    "models_list": {
        "glaive-coder_7b": {
            "model_type": "transformers",
            "model_id": "glaiveai/glaive-coder-7b",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_deteam_managerlim": { "start": "<s>[INST]\n<<SYS>>\n", "end": "\n<</SYS>>\n\n" },
            "model_usr_delim": { "start": ""                    , "end": "[/INST]"        }
        },
        "WizardCoder_13b": {
            "model_type": "transformers",
            "model_id": "WizardLM/WizardCoder-Python-13B-V1.0",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_delim": { "start": "### Instruction:\n", "end": "\n"            },
            "model_usr_delim": { "start": ""                  , "end": "### Response:" }
        },
        "WizardCoder_34b": {
            "model_type": "transformers",
            "model_id": "WizardLM/WizardCoder-Python-34B-V1.0",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_delim": { "start": ""                  , "end": "\n\n"                     },
            "model_usr_delim": { "start": "### Instruction:\n", "end": "\n\n### Response:"        }
        },
        "deepseek_coder_inst_6_7b": {
            "model_type": "transformers",
            "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_delim": { "start": ""                  , "end": "\n\n"                     },
            "model_usr_delim": { "start": "### Instruction:\n", "end": "\n\n### Response:"        }
        },
        "deepseek_coder_inst_1_3b": {
            "model_type": "transformers",
            "model_id": "deepseek-ai/deepseek-coder-1.3b-instruct",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_delim": { "start": ""                  , "end": "\n\n"                     },
            "model_usr_delim": { "start": "### Instruction:\n", "end": "\n\n### Response:"        }
        },
        "airophin_13b": {
            "model_type": "transformers",
            "model_id": "bhenrym14/airophin-13b-pntk-16k-fp16",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_delim": { "start": "<|im_start|>system\n", "end": "\n<|im_end|>\n"                        },
            "model_usr_delim": { "start": "<|im_start|>user\n"  , "end": "\n<|im_end|>\n<|im_start|>assistant\n" }
        },
        "Llama2_chat_13b": {
            "model_type": "transformers",
            "model_id": "posicube/Llama2-chat-AYT-13B",
            "model_bits": 4,
            "max_tokens": 4096,
            "model_sys_delim": { "start": "<s>[INST] <<SYS>>\n", "end": "\n<</SYS>>\n\n" },
            "model_usr_delim": { "start": ""                   , "end": "[/INST]"        }
        },
        "OpenOrca_7b": {
            "model_type": "transformers",
            "model_id": "Open-Orca/Mistral-7B-OpenOrca",
            "model_bits": 4,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "<|im_start|> system\n", "end": "\n<|im_end|>\n"        },
            "model_usr_delim": { "start": "<|im_start|>user\n"   , "end": "<|im_end|>\n"          }
        },
        "zephyr_a_7b": {
            "model_type": "transformers",
            "model_id": "HuggingFaceH4/zephyr-7b-alpha",
            "model_bits": 8,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "<|system|>\n", "end": "</s>\n"        },
            "model_usr_delim": { "start": "<|user|>\n"  , "end": "</s>\n"        }
        },        
        "zephyr_b_7b": {
            "model_type": "transformers",
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "model_bits": 8,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "<|system|>\n", "end": "</s>\n"        },
            "model_usr_delim": { "start": "<|user|>\n"  , "end": "</s>\n"        }
        },
        "dolphin_7b": {
            "model_type": "transformers",
            "model_id": "ehartford/dolphin-2.1-mistral-7b",
            "model_bits": 8,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "<|system|>\n", "end": "</s>\n"        },
            "model_usr_delim": { "start": "<|user|>\n"  , "end": "</s>\n"        }
        },
        "intel_7b": {
            "model_type": "transformers",
            "model_id": "Intel/neural-chat-7b-v3-1",
            "model_bits": 8,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "### System:\n", "end": "\n"        },
            "model_usr_delim": { "start": "### User:\n"  , "end": "\n"        }
        },
        "orca_2_13b": {
            "model_type": "transformers",
            "model_id": "microsoft/Orca-2-13b",
            "model_bits": 8,
            "max_tokens": 4096,
            "model_sys_delim": { "start": "<|im_start|>system\n", "end": "<|im_end|>\n" },
            "model_usr_delim": { "start": "<|im_start|>user\n"  , "end": "<|im_end|>\n" }
        },
        "openAI_gpt_4_turbo": {
            "model_type": "openAI",
            "model_id": "gpt-4-1106-preview",
            "max_tokens": 4096,
            "model_sys_delim": { "start": "System Instructions:\n", "end": "\n"          },
            "model_usr_delim": { "start": "User Instructions"     , "end": "\nResponse:" }
        } ,
        "openAI_gpt_4": {
            "model_type": "openAI",
            "model_id": "gpt-4",
            "max_tokens": 8192,
            "model_sys_delim": { "start": "System Instructions:\n", "end": "\n"          },
            "model_usr_delim": { "start": "User Instructions"     , "end": "\nResponse:" }
        },
        "openAI_gpt_3_5_turbo": {
            "model_type": "openAI",
            "model_id": "gpt-3.5-turbo-16k",
            "max_tokens": 16000,
            "model_sys_delim": { "start": "System Instructions:\n", "end": "\n"          },
            "model_usr_delim": { "start": "User Instructions"     , "end": "\nResponse:" }
        }
    },
    "vectorstore": {
        "type": "faiss",
        "path": "vectors",
        "chunk_size": 800,
        "chunk_overlap": 100
    },
    "actions": {
        "free": {
            "model_name": "intel_7b",
            "action_type": "generate",
            "temperature": 0.9,
            "max_new_tokens": 1024,
            "prompt_system": ["please use your best skills to perform the actions or demands requested by the usser"],
            "prompt_template":["{text}"],
            "llm_returns": "text"
        },
        "free_OpenAI": {
            "model_name": "openAI_gpt_4_turbo",
            "action_type": "generate",
            "temperature": 0.9,
            "max_new_tokens": 1024,
            "prompt_system": ["please use your best skills to perform the actions or demands requested by the usser"],
            "prompt_template":["{text}"],
            "llm_returns": "text"
        },
        "summarize": {
            "model_name": "zephyr_b_7b",
            "action_type": "generate",
            "chunkable_action": true,
            "temperature": 0.1,
            "prompt_system": [
                "you are the finest summarizer in the world, You manage to condense a text to it's quintessence preserving the informations",
                " and the style of the original text. You provide a summary where no words are wasted and no information is lost." ],
            "prompt_template":[
                "Please provide a summary of the following text enclosed in triple backticks.",
                "'''{text}''':\n"],
            "llm_returns": "text"
        },
        "summarize_OpenAI": {
            "model_name": "openAI_gpt_3_5_turbo",
            "action_type": "generate",
            "chunkable_action": true,
            "temperature": 0.1,
            "prompt_system": [
                "you are the finest summarizer in the world, You manage to condense a text to it's quintessence preserving the informations",
                " and the style of the original text. You provide a summary where no words are wasted and no information is lost." ],
            "prompt_template":[
                "Please provide a summary of the following text enclosed in triple backticks.",
                "'''{text}''':\n"],
            "llm_returns": "text"
        },
        "search": {
            "model_name": "zephyr_b_7b",
            "action_type": "generate",
            "action_post_function": "search",
            "temperature": 0.7,
            "prompt_system": [
                "you are the finest internet search expert in the world, You know all the tricks to find the most relevant information on the web",
                " and and you use this knowledge to craft the most relevant search queries to answer to the user demand" ],
            "prompt_template":[
                "Please craft a search querry that will allow you to get relevant information to answer the following demand which is provided",
                " enclosed in triple backticks.\n",
                "'''{text}'''\n"],
            "llm_returns": "text"
        },
        "split": {
            "model_name": "orca_2_13b",
            "action_type": "generate",
            "temperature": 0.01,
            "presence_penalty":  0.6,
            "prompt_system": [
                "You are an exceptional project manager, you are an expert at identifying the atomic actions to be performed in order to complete a task.",
                " you mostly split tasks in the following action_categories ['requirements', 'architecture', 'code', 'code_quality', 'search'] if it dose",
                " not fit within those action_categories you can use the 'split' action_category to further refine the task or if it dose not fit any of the",
                " previous you can use the 'free' action_category. Your output will always consist of your thought proces description followed by a list of",
                " json, each json object containing the following keys the 'objective' key where you detail the objective of the task, the 'action' key",
                " where you put an action from the action_categories list, and the associated 'prompt' key where you put the prompt required to perform this action." ],
            "prompt_template":[
                "Please generate the list of actions to be performed in order to complete task described within the following text enclosed in triple backticks.",
                "'''{text}'''\n"],
            "llm_returns": "actions_json"
        },
        "code": {
            "model_name": "WizardCoder_34b",
            "action_type": "generate",
            "code_language": "python",
            "temperature": 0.4,
            "prompt_system": [
                "you are an exceptional {code_language} coder, you produce {code_language} code that function as pre requirements and that is safe, easy to read",
                " and to understand for all coders, you respects the best practices of codding and documentation",
                " you always start your code by a comment describing your approach and thought process" ],
            "prompt_template": [
                "please generate the code required to satisfy the following demand enclosed in triple backticks.",
                "'''{text}'''\n"],
            "llm_returns": "code_text"               
        },
        "code_OpenAI": {
            "model_name": "openAI_gpt_4",
            "action_type": "generate",
            "code_language": "python",
            "temperature": 0.4,
            "prompt_system": [
                "you are an exceptional {code_language} coder, you produce {code_language} code that function as pre requirements and that is safe, easy to read",
                " and to understand for all coders, you respects the best practices of codding and documentation",
                " you always start your code by a comment describing your approach and thought process" ],
            "prompt_template": [
                "please generate the code required to satisfy the following demand enclosed in triple backticks.",
                "'''{text}'''\n"],
            "llm_returns": "code_text"               
        },
       "code_quality": {
            "model_name": "deepseek_coder_inst_6_7b",
            "action_type": "generate",
            "code_language": "python",
            "temperature": 0.1,
            "prompt_system": [
                "you are an exceptionaly sharp {code_language} quality expert, you analyze the provided code and ensure that it function as per requirements and that is",
                " safe, easy to read, and adequately documented and that it respects the best practices of codding and documentation. Your output will always",
                " consist of your thought proces description followed by a list of json, containg one json for each correrctives actions to be performed on the",
                " code to bring it to the highest standrds of quality each json will contain the following keys the 'objective' key where you detail the objective",
                " of the task, the 'action' key which will always be 'code_mod', the 'initial_code' where you copy the actual code that needs to be improved and",
                " the associated 'prompt' key where you put the prompt required to perform this code change." ],
            "prompt_template": [
                "Please analyse the following code provided within enclosed in triple backticks \n",
                "'''{text}'''\n"],
            "llm_returns": "actions_json"
        },
        "project_requirements": {
            "model_name": "orca_2_13b",
            "action_type": "generate",
            "code_language": "python",
            "max_new_tokens": 2048,
            "temperature": 0.5,
            "prompt_system": [
                "you are a talented and experienced {code_language} product owner, a large number of projects have been sucessfull thanks to you ability to",
                " define the right requirements for them. You ewperience enables you to analyze partial project informations and deduce the",
                " requirements that are needed to ensure the project output will function as intended and respects the best practices of {code_language}",
                " coding and documentation, one of your talents is to focus on the requirements that will bring the best code usefulness for the user",
                " and the best code quality bringing huge value to the project. Your other talent is to avoid un necessary requirements."],
            "prompt_template": [
                "Please generate requirements for the following the text delimited by triple backtick\n",
                "Start by describing your in depth analysys on the text to identify the requirement neededs",
                " you ensure requirements are described and well separated not mixing two or more requirements ionto one,single concept per requirements",
                "You will provide the your final set of requirements as a JSON, containg",
                " the following keys for each requirement: \n",
                "   'objective': where you detail the objective of the requirement,\n",
                "   'benefits': where you detail how this requirement benefits the project,\n",
                "   'requirement': where you describe the details of the requirement,\n",
                "   'validation_criteria' where you define the criteria to be able to validate this requirement.",
                "'''{text}'''\n"],
            "llm_returns": "requirements_json"
        },
        "team_manager": {
            "model_name": "zephyr_b_7b",
            "action_type": "generate",
            "max_new_tokens": 1024,
            "temperature": 0.6,
            "prompt_system": [
                "You are a awesome maager you are highly skilled with excellent human touch and value standards",
                " that enable you to motivate and protect your team you are able to recognise their work and",
                " uplift their spirit in any conditions.\n",
                "Managing your team's loading to ensure they a safe work environement. You have the right words",
                " and empathy to bring your team to exceptional motivation level even in adversity.\n",
                "Your strategic thinking enables you to planify the tasks to be performed in order to achieve",
                " the project objectives in time.\n"],
            "prompt_template": [
                "Please generate a speach for your tem to motivate them on the following subject\n",
                "'''{text}'''\n"],
            "llm_returns": "text"
        }

    }
}


